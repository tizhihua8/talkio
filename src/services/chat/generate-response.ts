import type {
  Conversation,
  Message,
  MessageRole,
  ChatApiMessage,
  ChatApiToolDef,
  Identity,
} from "../../types";
import { MessageStatus, MessageBlockType, MessageBlockStatus } from "../../types";
import { generateId } from "../../utils/id";
import { ApiClient } from "../api-client";
import {
  insertMessage,
  updateMessage as dbUpdateMessage,
  updateConversation as dbUpdateConversation,
  getRecentMessages as dbGetRecentMessages,
  insertBlock,
  updateBlock as dbUpdateBlock,
} from "../../storage/database";
import { batchUpdateMessage, flushBatchUpdates } from "../../storage/batch-writer";
import { batchUpdateBlock, flushBlockBatchUpdates } from "../../storage/block-batch-writer";
import { useProviderStore } from "../../stores/provider-store";
import { useIdentityStore } from "../../stores/identity-store";
import { useChatStore } from "../../stores/chat-store";
import { logger } from "../logger";
import { buildApiMessages, autoGenerateTitle } from "./message-builder";
import { buildTools, executeToolCalls } from "./tool-executor";

const log = logger.withContext("ChatService");

export async function generateResponse(
  conversationId: string,
  modelId: string,
  conv: Conversation,
  signal?: AbortSignal,
): Promise<void> {
  const providerStore = useProviderStore.getState();
  const identityStore = useIdentityStore.getState();
  const chatStore = useChatStore.getState();

  const model = providerStore.getModelById(modelId);
  if (!model) {
    log.warn(`Model not found: ${modelId}`);
    return;
  }

  const provider = providerStore.getProviderById(model.providerId);
  if (!provider) {
    log.warn(`Provider not found: ${model.providerId}`);
    return;
  }

  const participant = conv.participants.find((p) => p.modelId === modelId);
  const identity: Identity | undefined = participant?.identityId
    ? identityStore.getIdentityById(participant.identityId)
    : undefined;

  // Read messages from DB (single source of truth)
  const dbMessages = await dbGetRecentMessages(conversationId, chatStore.activeBranchId, 100);
  const apiMessages = await buildApiMessages(dbMessages, modelId, identity);

  // Create assistant message FIRST so loading animation appears immediately
  const assistantMsg: Message = {
    id: generateId(),
    conversationId,
    role: "assistant",
    senderModelId: modelId,
    senderName: model.displayName,
    identityId: identity?.id ?? null,
    content: "",
    images: [],
    generatedImages: [],
    reasoningContent: null,
    reasoningDuration: null,
    toolCalls: [],
    toolResults: [],
    branchId: chatStore.activeBranchId,
    parentMessageId: null,
    isStreaming: true,
    status: MessageStatus.STREAMING,
    errorMessage: null,
    tokenUsage: null,
    createdAt: new Date().toISOString(),
  };

  await insertMessage(assistantMsg);

  // Create initial MAIN_TEXT block for the assistant message
  const mainTextBlockId = generateId();
  let thinkingBlockId: string | null = null;
  await insertBlock({
    id: mainTextBlockId,
    messageId: assistantMsg.id,
    type: MessageBlockType.MAIN_TEXT,
    content: "",
    status: MessageBlockStatus.STREAMING,
    metadata: null,
    sortOrder: 1,
    createdAt: assistantMsg.createdAt,
    updatedAt: null,
  });

  // Discover tools AFTER showing loading animation
  log.info(`[generateResponse] Building tools for ${model.displayName}...`);
  let tools: ChatApiToolDef[] = [];
  try {
    tools = await buildTools(model, identity);
  } catch (err) {
    log.warn(`[generateResponse] buildTools failed, proceeding without tools: ${err instanceof Error ? err.message : err}`);
  }
  log.info(`[generateResponse] Tools ready: ${tools.length} tools`);

  const contentChunks: string[] = [];
  const reasoningChunks: string[] = [];
  let contentLen = 0;
  let reasoningLen = 0;
  const generatedImages: string[] = [];
  const pendingToolCalls: Array<{
    id: string;
    name: string;
    arguments: string;
  }> = [];

  // Helper: always mark message as done (isStreaming=false), never throws
  const finishMessage = async (finalContent: string) => {
    try {
      await dbUpdateMessage(assistantMsg.id, { content: finalContent, isStreaming: false });
    } catch (e) {
      log.error(`DB update failed: ${e}`);
    }
  };

  try {
    const client = new ApiClient(provider);

    // Build reasoning params if model supports it
    const effort = identity?.params.reasoningEffort ?? "auto";
    const reasoningParams: Record<string, unknown> = {};
    if (model.capabilities.reasoning && effort !== "none") {
      const mid = model.modelId.toLowerCase();
      if (mid.includes("claude") && provider.type === "anthropic") {
        // Native Anthropic API only — proxies use <think> tags instead
        const budgetMap = { low: 4096, medium: 8192, high: 16384, auto: 8192 };
        reasoningParams.thinking = { type: "enabled", budget_tokens: budgetMap[effort] ?? 8192 };
      } else if (mid.includes("hunyuan")) {
        reasoningParams.enable_thinking = true;
      } else {
        // Gemini, o1/o3/o4, Grok, etc. — use reasoning_effort for OpenAI-compatible proxies
        reasoningParams.reasoning_effort = effort === "auto" ? "medium" : effort;
      }
    }

    // o1/o3/o4 models don't support temperature, top_p, or system role
    const isOModel = /\b(o1|o3|o4)\b/.test(model.modelId.toLowerCase());
    // Claude/Gemini: temperature 0..1; OpenAI: 0..2
    const mid = model.modelId.toLowerCase();
    const maxTemp = (mid.includes("claude") || mid.includes("gemini")) ? 1 : 2;
    let temperature = identity?.params.temperature;
    if (temperature !== undefined) {
      temperature = Math.min(Math.max(temperature, 0), maxTemp);
    }
    const streamParams: Record<string, unknown> = {
      model: model.modelId,
      messages: isOModel
        ? apiMessages.map((m) =>
            m.role === "system" ? { ...m, role: "user" as const, content: `[System Instructions]\n${typeof m.content === "string" ? m.content : ""}` } : m,
          )
        : apiMessages,
      stream: true,
      ...(isOModel ? {} : {
        temperature,
        top_p: identity?.params.topP,
      }),
      tools: tools.length > 0 ? tools : undefined,
      ...reasoningParams,
    };
    // Remove undefined values to avoid sending them to the API
    for (const key of Object.keys(streamParams)) {
      if (streamParams[key] === undefined) delete streamParams[key];
    }
    log.info(`[generateResponse] Starting stream for ${model.modelId} to ${provider.baseUrl}...`);
    const stream = client.streamChat(streamParams as any, signal);

    let inThinkTag = false;

    // Process a text chunk: handle <think> tags
    // Uses array buffers instead of string concatenation to reduce GC pressure
    const processTextChunk = (raw: string) => {
      let chunk = raw;
      while (chunk) {
        if (inThinkTag) {
          const closeIdx = chunk.indexOf("</think>");
          if (closeIdx !== -1) {
            const part = chunk.slice(0, closeIdx);
            reasoningChunks.push(part);
            reasoningLen += part.length;
            chunk = chunk.slice(closeIdx + 8);
            inThinkTag = false;
          } else {
            reasoningChunks.push(chunk);
            reasoningLen += chunk.length;
            chunk = "";
          }
        } else {
          const openIdx = chunk.indexOf("<think>");
          if (openIdx !== -1) {
            const part = chunk.slice(0, openIdx);
            contentChunks.push(part);
            contentLen += part.length;
            chunk = chunk.slice(openIdx + 7);
            inThinkTag = true;
          } else {
            contentChunks.push(chunk);
            contentLen += chunk.length;
            chunk = "";
          }
        }
      }
    };

    let chunkCount = 0;
    let uiDirty = false;
    let flushTimer: ReturnType<typeof setTimeout> | null = null;
    const UI_THROTTLE_MS = 120;
    const MAX_FLUSH_DELAY_MS = 240;
    const MIN_CHARS_TO_FLUSH = 32;
    let lastFlushAt = 0;
    let lastFlushedContentLength = 0;
    let lastFlushedReasoningLength = 0;
    let lastFlushedToolCallsLength = 0;
    let lastFlushedImagesLength = 0;
    let cachedToolCalls: Array<{ id: string; name: string; arguments: string }> = [];
    let cachedImages: string[] = [];

    const shouldFlush = () => {
      const now = Date.now();
      if (generatedImages.length !== lastFlushedImagesLength) return true;
      if (pendingToolCalls.length !== lastFlushedToolCallsLength) return true;
      const contentDelta = contentLen - lastFlushedContentLength;
      const reasoningDelta = reasoningLen - lastFlushedReasoningLength;
      return (
        contentDelta >= MIN_CHARS_TO_FLUSH ||
        reasoningDelta >= MIN_CHARS_TO_FLUSH ||
        now - lastFlushAt >= MAX_FLUSH_DELAY_MS
      );
    };

    const flushUI = () => {
      flushTimer = null;
      if (!uiDirty) return;
      if (!shouldFlush()) {
        scheduleFlush();
        return;
      }
      uiDirty = false;
      lastFlushAt = Date.now();
      lastFlushedContentLength = contentLen;
      lastFlushedReasoningLength = reasoningLen;
      lastFlushedToolCallsLength = pendingToolCalls.length;
      lastFlushedImagesLength = generatedImages.length;
      // Only rebuild arrays when they actually changed
      if (pendingToolCalls.length !== cachedToolCalls.length) {
        cachedToolCalls = pendingToolCalls.map((tc) => ({
          id: tc.id,
          name: tc.name,
          arguments: tc.arguments,
        }));
      }
      if (generatedImages.length !== cachedImages.length) {
        cachedImages = [...generatedImages];
      }
      // Batch streaming content to DB — merged + flushed every 180ms
      const joinedContent = contentChunks.join("");
      const joinedReasoning = reasoningChunks.join("");
      batchUpdateMessage(assistantMsg.id, {
        content: joinedContent,
        generatedImages: cachedImages,
        reasoningContent: joinedReasoning || null,
        toolCalls: cachedToolCalls,
      });
      // Update MAIN_TEXT block
      batchUpdateBlock(mainTextBlockId, {
        content: joinedContent,
        status: MessageBlockStatus.STREAMING,
      });
      // Create or update THINKING block if reasoning content exists
      if (joinedReasoning) {
        if (!thinkingBlockId) {
          thinkingBlockId = generateId();
          insertBlock({
            id: thinkingBlockId,
            messageId: assistantMsg.id,
            type: MessageBlockType.THINKING,
            content: joinedReasoning,
            status: MessageBlockStatus.STREAMING,
            metadata: null,
            sortOrder: 0,
            createdAt: new Date().toISOString(),
            updatedAt: null,
          }).catch((e) => log.error(`Insert thinking block failed: ${e}`));
        } else {
          batchUpdateBlock(thinkingBlockId, {
            content: joinedReasoning,
            status: MessageBlockStatus.STREAMING,
          });
        }
      }
    };

    const scheduleFlush = () => {
      uiDirty = true;
      if (!flushTimer) flushTimer = setTimeout(flushUI, UI_THROTTLE_MS);
    };

    for await (const delta of stream) {
      chunkCount++;
      if (chunkCount === 1) log.info(`[generateResponse] First chunk received`);
      // Handle content: can be string or multimodal array
      if (delta.content != null) {
        if (typeof delta.content === "string") {
          processTextChunk(delta.content);
        } else if (Array.isArray(delta.content)) {
          for (const part of delta.content) {
            if (part.type === "text" && part.text) {
              processTextChunk(part.text);
            } else if (part.type === "image_url" && part.image_url?.url) {
              generatedImages.push(part.image_url.url);
            }
          }
        }
      }
      // Direct reasoning_content field (DeepSeek R1, etc.)
      if (delta.reasoning_content) {
        reasoningChunks.push(delta.reasoning_content);
        reasoningLen += delta.reasoning_content.length;
      }
      // Some providers use 'reasoning' field
      if ((delta as any).reasoning) {
        const r = (delta as any).reasoning;
        reasoningChunks.push(r);
        reasoningLen += r.length;
      }
      if (delta.tool_calls) {
        for (const tc of delta.tool_calls) {
          if (!pendingToolCalls[tc.index]) {
            pendingToolCalls[tc.index] = {
              id: tc.id ?? "",
              name: tc.function?.name ?? "",
              arguments: "",
            };
          }
          if (tc.id) pendingToolCalls[tc.index].id = tc.id;
          if (tc.function?.name) pendingToolCalls[tc.index].name = tc.function.name;
          if (tc.function?.arguments) {
            pendingToolCalls[tc.index].arguments += tc.function.arguments;
          }
        }
      }

      scheduleFlush();
    }

    // Final flush after stream ends
    if (flushTimer) clearTimeout(flushTimer);
    if (uiDirty) flushUI();

    // Join chunk arrays into final strings
    let content = contentChunks.join("");
    const reasoningContent = reasoningChunks.join("");

    // Post-stream: extract markdown images from content
    const mdImageRegex = /!\[[^\]]*\]\((data:image\/[^)]+)\)/g;
    let match: RegExpExecArray | null;
    while ((match = mdImageRegex.exec(content)) !== null) {
      generatedImages.push(match[1]);
    }
    if (generatedImages.length > 0) {
      content = content.replace(/!\[[^\]]*\]\(data:image\/[^)]+\)/g, "").trim();
    }

    const toolCallsSnapshot = pendingToolCalls.map((tc) => ({
      id: tc.id,
      name: tc.name,
      arguments: tc.arguments,
    }));
    let toolResults: Array<{ toolCallId: string; content: string }> = [];

    if (pendingToolCalls.length > 0) {
      toolResults = await executeToolCalls(pendingToolCalls);
      batchUpdateMessage(assistantMsg.id, { toolResults });

      const assistantApiMsg: ChatApiMessage = {
        role: "assistant",
        content: content || "",
        tool_calls: toolCallsSnapshot.map((tc) => ({
          id: tc.id,
          type: "function" as const,
          function: { name: tc.name, arguments: tc.arguments },
        })),
      };
      const toolApiMsgs: ChatApiMessage[] = toolResults.map((tr) => ({
        role: "tool" as MessageRole,
        content: tr.content,
        tool_call_id: tr.toolCallId,
      }));
      const followUpMessages: ChatApiMessage[] = [
        ...apiMessages,
        assistantApiMsg,
        ...toolApiMsgs,
      ];

      const followUpParams: Record<string, unknown> = {
        model: model.modelId,
        messages: followUpMessages,
        stream: true,
        ...(isOModel ? {} : {
          temperature,
          top_p: identity?.params.topP,
        }),
      };
      for (const key of Object.keys(followUpParams)) {
        if (followUpParams[key] === undefined) delete followUpParams[key];
      }
      const followUpStream = client.streamChat(followUpParams as any, signal);

      let followUpContent = "";
      let fuDirty = false;
      let fuTimer: ReturnType<typeof setTimeout> | null = null;
      const flushFollowUp = () => {
        fuTimer = null;
        fuDirty = false;
        batchUpdateMessage(assistantMsg.id, {
          content: followUpContent || content,
        });
      };
      for await (const delta of followUpStream) {
        if (delta.content) {
          followUpContent += delta.content;
        }
        fuDirty = true;
        if (!fuTimer) fuTimer = setTimeout(flushFollowUp, UI_THROTTLE_MS);
      }
      if (fuTimer) clearTimeout(fuTimer);
      if (fuDirty) flushFollowUp();

      if (followUpContent) {
        content = followUpContent;
      }
    }

    // Flush any pending batched updates before final commit
    await flushBatchUpdates([assistantMsg.id]);
    const blockIdsToFlush = [mainTextBlockId];
    if (thinkingBlockId) blockIdsToFlush.push(thinkingBlockId);
    await flushBlockBatchUpdates(blockIdsToFlush);

    await dbUpdateMessage(assistantMsg.id, {
      content,
      generatedImages,
      reasoningContent: reasoningContent || null,
      toolCalls: toolCallsSnapshot,
      toolResults,
      isStreaming: false,
      status: MessageStatus.SUCCESS,
    });
    // Finalize blocks
    await dbUpdateBlock(mainTextBlockId, { content, status: MessageBlockStatus.SUCCESS });
    if (thinkingBlockId && reasoningContent) {
      await dbUpdateBlock(thinkingBlockId, { content: reasoningContent, status: MessageBlockStatus.SUCCESS });
    }

    // Commit: finalize message in DB — useLiveQuery handles UI automatically
    const now = new Date().toISOString();

    dbUpdateConversation(conversationId, {
      lastMessage: content.slice(0, 100),
      lastMessageAt: now,
    }).catch((e) => log.error(`DB conversation update failed: ${e}`));

    // Auto-generate conversation title on first response
    log.info(`[generateResponse] Stream complete. ${chunkCount} chunks, ${content.length} chars`);
    autoGenerateTitle(conversationId, client, model, dbMessages, content).catch(() => {});
  } catch (err) {
    let errMsg: string;
    if (err instanceof Error) {
      errMsg = err.message || err.name || "Unknown Error";
    } else if (typeof err === "string") {
      errMsg = err;
    } else {
      try { errMsg = JSON.stringify(err); } catch { errMsg = String(err); }
    }
    const errStack = err instanceof Error ? err.stack : '';
    // Use warn so Metro's HMR client doesn't swallow the message
    console.warn(`⚠️ Stream error [${model.displayName}]: ${errMsg}`);
    log.error(`Stream error for ${model.displayName}: ${errMsg}\n${errStack}`);

    if (signal?.aborted) {
      await finishMessage(contentChunks.join("") || "(stopped)");
      // Mark as paused (user-initiated stop)
      dbUpdateMessage(assistantMsg.id, { status: MessageStatus.PAUSED }).catch(() => {});
      return;
    }
    await finishMessage(`[${model.displayName}] Error: ${errMsg}`);
    // Persist error status and message
    dbUpdateMessage(assistantMsg.id, {
      status: MessageStatus.ERROR,
      errorMessage: errMsg,
    }).catch(() => {});
  }
}
